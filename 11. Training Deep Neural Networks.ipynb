{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di Bab 10, kita telah melatih jaringan saraf yang relatif dangkal. Namun, untuk masalah yang sangat kompleks, Anda mungkin perlu melatih Jaringan Saraf Dalam (Deep Neural Networks - DNNs) yang memiliki puluhan atau bahkan ratusan lapisan. Proses pelatihan ini tidaklah mudah dan seringkali dihadapkan pada beberapa masalah:\n",
    "\n",
    "* **Vanishing/Exploding Gradients Problem:** Gradien menjadi sangat kecil atau sangat besar saat *backpropagation*, membuat lapisan bawah sulit untuk dilatih.\n",
    "* **Kurangnya Data Pelatihan:** Jaringan besar memerlukan banyak data.\n",
    "* **Pelatihan yang Lambat:** Jaringan besar bisa sangat lambat untuk dilatih.\n",
    "* **Risiko Overfitting:** Model dengan jutaan parameter sangat berisiko *overfitting*.\n",
    "\n",
    "Bab ini akan membahas setiap masalah ini dan menyajikan teknik-teknik untuk mengatasinya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masalah ini terjadi karena gradien seringkali menjadi semakin kecil (vanishing) atau semakin besar (exploding) saat algoritma bergerak mundur melalui lapisan-lapisan DNN. Beberapa solusi yang populer adalah:\n",
    "\n",
    "### 1. Glorot and He Initialization\n",
    "Inisialisasi bobot koneksi secara acak dengan cara yang benar sangat penting. Strategi inisialisasi Glorot (Xavier) dan He membantu menjaga varians sinyal tetap konstan saat bergerak maju dan mundur melalui jaringan, yang secara signifikan mengurangi risiko gradien yang menghilang atau meledak.\n",
    "\n",
    "### 2. Nonsaturating Activation Functions\n",
    "Fungsi aktivasi seperti Sigmoid dan Tanh cenderung \"jenuh\" di nilai ekstrim, membuat turunannya mendekati nol. Ini menyebabkan gradien berhenti mengalir. Solusinya adalah menggunakan fungsi aktivasi yang tidak jenuh, seperti **ReLU** dan variannya (**Leaky ReLU, ELU, SELU**).\n",
    "\n",
    "### 3. Batch Normalization (BN)\n",
    "Teknik ini menambahkan operasi di setiap lapisan yang me-nol-pusatkan dan menormalkan inputnya, lalu menskalakan dan menggesernya. Ini secara dramatis mengurangi masalah gradien yang tidak stabil dan memungkinkan penggunaan *learning rate* yang lebih tinggi, mempercepat konvergensi.\n",
    "\n",
    "### 4. Gradient Clipping\n",
    "Teknik lain untuk mengatasi gradien yang meledak adalah dengan \"memotong\" gradien selama *backpropagation* agar tidak pernah melebihi ambang batas tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Contoh penggunaan inisialisasi He dan Batch Normalization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melatih DNN yang sangat besar dari awal bukanlah ide yang baik. Sebaliknya, kita harus selalu mencoba mencari jaringan saraf yang sudah ada yang menyelesaikan tugas serupa, lalu menggunakan kembali lapisan bawahnya. Teknik ini disebut **Transfer Learning**.\n",
    "\n",
    "Prosesnya melibatkan:\n",
    "1. Memuat model yang sudah dilatih sebelumnya (misalnya, dilatih pada dataset ImageNet).\n",
    "2. Membekukan bobot lapisan-lapisan bawahnya agar tidak berubah selama pelatihan awal.\n",
    "3. Mengganti lapisan output dengan lapisan output baru yang sesuai dengan tugas kita.\n",
    "4. Melatih model pada data kita. Lapisan baru akan belajar dengan cepat, sementara lapisan yang dibekukan tidak akan berubah.\n",
    "5. (Opsional) Membuka beberapa lapisan beku dan melanjutkan pelatihan dengan *learning rate* yang lebih rendah untuk menyempurnakan model (*fine-tuning*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Transfer Learning (konseptual)\n",
    "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Membekukan lapisan yang digunakan kembali\n",
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# Melatih model (hanya lapisan baru yang akan dilatih)\n",
    "# model_B_on_A.compile(...)\n",
    "# history = model_B_on_A.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selain SGD biasa, ada beberapa *optimizer* yang lebih canggih yang dapat mempercepat pelatihan secara signifikan:\n",
    "\n",
    "* **Momentum Optimization:** Menambahkan \"momentum\" pada pembaruan bobot, membantunya melewati *plateau* dan osilasi.\n",
    "* **Nesterov Accelerated Gradient (NAG):** Varian dari momentum yang sedikit lebih cepat.\n",
    "* **AdaGrad:** Memiliki *learning rate* yang adaptif, memberikan pembaruan yang lebih kecil untuk fitur yang sering muncul.\n",
    "* **RMSProp:** Memperbaiki kelemahan AdaGrad yang cenderung berhenti terlalu cepat.\n",
    "* **Adam & Nadam:** Menggabungkan ide dari momentum dan RMSProp. Adam adalah *optimizer* yang sangat populer dan sering menjadi pilihan *default* yang baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan optimizer Adam di Keras\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "# model.compile(loss=\"...\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menemukan *learning rate* yang baik sangat penting. Alih-alih menggunakan *learning rate* konstan, kita bisa mendapatkan solusi yang lebih baik dan lebih cepat dengan mengubahnya selama pelatihan. Strategi ini disebut **learning rate scheduling**. Beberapa jadwal umum termasuk *power scheduling*, *exponential scheduling*, dan *piecewise constant scheduling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN yang besar sangat rentan terhadap *overfitting*. Selain *early stopping* dan *Batch Normalization*, ada teknik regularisasi lain yang populer:\n",
    "\n",
    "* **L1 and L2 Regularization:** Menambahkan penalti pada *loss function* berdasarkan norma L1 atau L2 dari bobot koneksi. Ini mendorong bobot untuk tetap kecil.\n",
    "\n",
    "* **Dropout:** Pada setiap langkah pelatihan, setiap neuron (kecuali di *output layer*) memiliki probabilitas `p` untuk \"dijatuhkan\" (diabaikan sementara). Ini memaksa neuron lain untuk belajar fitur yang lebih kuat dan tidak terlalu bergantung satu sama lain, menghasilkan jaringan yang lebih tangguh.\n",
    "\n",
    "* **Max-Norm Regularization:** Membatasi norma L2 dari bobot koneksi yang masuk ke setiap neuron agar tidak melebihi nilai `r` tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh model dengan Dropout\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
