{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bab ini membahas **Ensemble Learning**, sebuah teknik di mana prediksi dari sekelompok prediktor (seperti *classifier* atau *regressor*) digabungkan untuk mendapatkan prediksi yang lebih baik daripada prediktor individual terbaik sekalipun. Sekelompok prediktor ini disebut *ensemble*.\n",
    "\n",
    "Sebagai contoh, sebuah *ensemble* dari Decision Trees disebut **Random Forest**. Meskipun sederhana, ini adalah salah satu algoritma Machine Learning yang paling kuat yang tersedia saat ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara sederhana untuk membuat *classifier* yang lebih baik adalah dengan menggabungkan prediksi dari beberapa *classifier* dan memprediksi kelas yang mendapatkan suara terbanyak. Ini disebut **voting classifier**.\n",
    "\n",
    "* **Hard Voting:** Memprediksi kelas yang paling banyak dipilih oleh *classifier* individual.\n",
    "* **Soft Voting:** Memprediksi kelas dengan probabilitas kelas rata-rata tertinggi. Ini seringkali berkinerja lebih baik karena memberikan bobot lebih pada suara yang sangat yakin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42, probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendekatan lain adalah menggunakan algoritma pelatihan yang sama untuk setiap prediktor tetapi melatihnya pada subset acak yang berbeda dari set pelatihan.\n",
    "\n",
    "* **Bagging (Bootstrap Aggregating):** Pengambilan sampel dilakukan **dengan** penggantian (*with replacement*).\n",
    "* **Pasting:** Pengambilan sampel dilakukan **tanpa** penggantian (*without replacement*).\n",
    "\n",
    "Setelah semua prediktor dilatih, *ensemble* dapat membuat prediksi dengan menggabungkan prediksi dari semua prediktor (biasanya menggunakan *mode* statistik untuk klasifikasi atau rata-rata untuk regresi).\n",
    "\n",
    "### Out-of-Bag (oob) Evaluation\n",
    "Dengan *bagging*, beberapa *instance* mungkin tidak pernah terpilih untuk pelatihan pada prediktor tertentu. *Instance* ini disebut *out-of-bag (oob)*. Kita dapat menggunakan *instance oob* ini untuk mengevaluasi kinerja *ensemble* tanpa memerlukan *validation set* terpisah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(bag_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** adalah sebuah *ensemble* dari Decision Trees, umumnya dilatih melalui metode *bagging*.\n",
    "\n",
    "Random Forest memperkenalkan keacakan ekstra saat menumbuhkan pohon: alih-alih mencari fitur terbaik saat membagi sebuah *node*, ia mencari fitur terbaik di antara subset acak dari fitur-fitur. Ini menghasilkan keragaman pohon yang lebih besar, yang menukar *bias* yang lebih tinggi dengan *variance* yang lebih rendah, menghasilkan model yang secara keseluruhan lebih baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting** mengacu pada metode *ensemble* apa pun yang dapat menggabungkan beberapa *weak learners* menjadi satu *strong learner*. Ide umumnya adalah melatih prediktor secara berurutan, masing-masing mencoba memperbaiki pendahulunya.\n",
    "\n",
    "### AdaBoost (Adaptive Boosting)\n",
    "Prediktor baru memberi perhatian lebih pada *instance* pelatihan yang salah diklasifikasikan oleh prediktor sebelumnya. Ini dicapai dengan meningkatkan bobot relatif dari *instance* yang salah diklasifikasikan.\n",
    "\n",
    "### Gradient Boosting\n",
    "Metode ini mencoba memasang prediktor baru pada *residual errors* (kesalahan sisa) yang dibuat oleh prediktor sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Contoh AdaBoost\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Contoh Gradient Boosting (untuk regresi)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, learning_rate=0.1)\n",
    "# gbrt.fit(X, y) # Perlu data regresi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking** (*stacked generalization*) adalah metode *ensemble* yang didasarkan pada ide sederhana: alih-alih menggunakan fungsi sepele (seperti voting) untuk menggabungkan prediksi dari semua prediktor dalam *ensemble*, mengapa kita tidak melatih sebuah model untuk melakukan agregasi ini?\n",
    "\n",
    "Prosesnya melibatkan pemisahan *training set*. *Layer* pertama dari prediktor dilatih pada satu bagian, kemudian membuat prediksi pada bagian lain. Prediksi-prediksi ini kemudian digunakan sebagai fitur masukan untuk melatih model akhir yang disebut **blender** atau **meta-learner**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
