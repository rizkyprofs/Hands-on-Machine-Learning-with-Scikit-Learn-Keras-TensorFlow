{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bab ini memperkenalkan **Reinforcement Learning (RL)**, salah satu bidang Machine Learning yang paling menarik. Berbeda dengan *supervised* atau *unsupervised learning*, RL berfokus pada bagaimana sebuah **agent** harus mengambil **tindakan** di dalam sebuah **lingkungan** untuk memaksimalkan total **reward** kumulatif.\n",
    "\n",
    "RL telah mencapai kesuksesan luar biasa dalam berbagai aplikasi, mulai dari mengalahkan juara dunia dalam permainan seperti Go (AlphaGo) hingga mengoptimalkan sistem yang kompleks.\n",
    "\n",
    "Kita akan membahas:\n",
    "* Konsep dasar RL: *agent*, *environment*, *rewards*, dan *policy*.\n",
    "* Dua teknik utama dalam Deep Reinforcement Learning: **Policy Gradients (PG)** dan **Deep Q-Networks (DQNs)**.\n",
    "* **Markov Decision Processes (MDPs)** sebagai kerangka kerja matematis untuk RL.\n",
    "* Penggunaan pustaka **OpenAI Gym** untuk lingkungan simulasi.\n",
    "* Penggunaan pustaka **TF-Agents** untuk membangun sistem RL yang kuat dengan TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma yang digunakan *agent* untuk menentukan tindakannya disebut **policy**. *Policy* ini bisa berupa apa saja, dari aturan sederhana hingga jaringan saraf yang kompleks. **Policy Search** adalah pendekatan di mana kita mencari parameter terbaik untuk sebuah *policy* yang akan memaksimalkan *reward*.\n",
    "\n",
    "Salah satu pendekatan yang paling populer adalah **Policy Gradients (PG)**. Dalam PG, kita menggunakan Gradient Descent untuk men-tweak parameter *policy* ke arah yang akan meningkatkan *reward*. Ini bekerja dengan cara:\n",
    "1. Menjalankan *policy* beberapa kali dan mengumpulkan hasilnya (*rewards*).\n",
    "2. Menghitung gradien yang akan membuat tindakan-tindakan yang menghasilkan *reward* tinggi lebih mungkin terjadi.\n",
    "3. Memperbarui parameter *policy* menggunakan gradien tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melatih *agent*, kita memerlukan lingkungan yang berfungsi. **OpenAI Gym** adalah *toolkit* yang menyediakan berbagai lingkungan simulasi (misalnya, game Atari, simulasi fisika) yang menjadi standar untuk penelitian RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Membuat lingkungan CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "\n",
    "# Menjalankan satu episode dengan policy acak\n",
    "for step in range(200):\n",
    "    action = env.action_space.sample() # memilih tindakan acak\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks (DQNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alih-alih secara langsung mengoptimalkan *policy*, pendekatan lain adalah dengan mempelajari nilai dari setiap tindakan. **Q-Learning** adalah algoritma yang belajar memperkirakan nilai optimal dari setiap pasangan (state, action), yang disebut **Q-Value**.\n",
    "\n",
    "**Deep Q-Network (DQN)** adalah implementasi dari Q-Learning menggunakan jaringan saraf dalam untuk mengaproksimasi Q-Values. Ini sangat efektif untuk masalah dengan ruang *state* yang sangat besar (misalnya, input dari piksel layar game).\n",
    "\n",
    "Teknik kunci dalam DQN adalah penggunaan **replay buffer** (atau *replay memory*), di mana pengalaman (*state, action, reward, next_state*) disimpan dan kemudian di-sampel secara acak untuk melatih jaringan. Ini membantu mengurangi korelasi antar pengalaman dan menstabilkan pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Konseptual: Membangun DQN untuk CartPole\n",
    "input_shape = [4] # obs: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
    "n_outputs = 2 # actions: [left, right]\n",
    "\n",
    "model_dqn = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TF-Agents Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengimplementasikan algoritma RL dari awal bisa sangat rumit. **TF-Agents** adalah pustaka dari Google yang menyediakan komponen-komponen yang dapat digunakan kembali dan teruji untuk membangun sistem RL yang kuat dan dapat diskalakan.\n",
    "\n",
    "TF-Agents menyediakan lingkungan, agen (yang mengimplementasikan algoritma seperti DQN dan PPO), *replay buffer*, metrik, dan *driver* untuk menjalankan loop interaksi antara *agent* dan *environment*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Pelatihan RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrik kinerja yang paling penting dalam RL adalah *reward* total yang didapat per episode. Memplot metrik ini dari waktu ke waktu menunjukkan apakah *agent* kita benar-benar belajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data reward dummy dari proses pelatihan (konseptual)\n",
    "episodes = np.arange(1, 601)\n",
    "# Reward awal rendah dan tidak stabil, kemudian meningkat tajam\n",
    "rewards_part1 = np.random.normal(loc=20, scale=8, size=300)\n",
    "rewards_part2 = np.random.normal(loc=190, scale=15, size=200).clip(min=100, max=200)\n",
    "rewards_part3 = np.random.normal(loc=50, scale=10, size=100) # Contoh catastrophic forgetting\n",
    "rewards = np.concatenate([rewards_part1, rewards_part2, rewards_part3])\n",
    "rewards = np.clip(rewards, 0, 200)\n",
    "\n",
    "# Menghitung rata-rata bergerak untuk memperhalus kurva\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size), 'valid') / window_size\n",
    "\n",
    "smoothed_rewards = moving_average(rewards, 30)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, rewards, label='Reward per Episode', alpha=0.3)\n",
    "plt.plot(episodes[len(episodes)-len(smoothed_rewards):], smoothed_rewards, 'r-', label='Rata-rata Bergerak (30 Episode)')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Kurva Pembelajaran Agent RL\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
