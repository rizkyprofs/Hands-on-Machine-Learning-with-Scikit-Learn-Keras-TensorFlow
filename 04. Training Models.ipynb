{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bab ini membuka \"kotak hitam\" dari beberapa model Machine Learning. Kita akan melihat secara mendalam bagaimana model-model ini bekerja, yang akan membantu kita dalam memilih model yang tepat, algoritma pelatihan yang benar, dan set *hyperparameter* yang baik untuk sebuah tugas. Pemahaman ini juga krusial untuk melakukan *debugging* dan analisis kesalahan.\n",
    "\n",
    "Fokus utama bab ini adalah pada **Regresi Linear**, menjelajahi dua cara yang sangat berbeda untuk melatihnya:\n",
    "1. Menggunakan persamaan matematis langsung (*closed-form equation*) yang disebut **Normal Equation**.\n",
    "2. Menggunakan pendekatan optimisasi iteratif yang disebut **Gradient Descent (GD)**, beserta variannya (Batch GD, Stochastic GD, dan Mini-batch GD).\n",
    "\n",
    "Kita juga akan membahas **Regresi Polinomial** untuk data non-linear, cara mendeteksi *overfitting* menggunakan **learning curves**, dan berbagai teknik **regularisasi** (Ridge, Lasso, Elastic Net) untuk menguranginya. Terakhir, kita akan melihat model klasifikasi seperti **Regresi Logistik** dan **Regresi Softmax**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model regresi linear membuat prediksi dengan menghitung jumlah berbobot dari fitur-fitur masukan, ditambah sebuah konstanta yang disebut *bias term* (atau *intercept term*).\n",
    "\n",
    "**Persamaan Prediksi Regresi Linear**\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n $$\n",
    "\n",
    "Melatih model ini berarti mengatur parameternya ($\\theta$) sehingga model paling sesuai dengan data pelatihan. Ukuran kinerja yang paling umum untuk regresi adalah **Mean Squared Error (MSE)**. Jadi, tujuan kita adalah menemukan nilai $\\theta$ yang meminimalkan MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation adalah formula matematis yang memberikan hasil secara langsung untuk menemukan nilai $\\theta$ yang meminimalkan fungsi biaya.\n",
    "\n",
    "**Normal Equation**\n",
    "$$ \\hat{\\theta} = (X^T X)^{-1} X^T y $$\n",
    "\n",
    "Ini adalah cara yang efisien untuk mendapatkan parameter optimal secara langsung, tetapi bisa menjadi sangat lambat jika jumlah fitur sangat besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Membuat data linear acak\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Menambahkan x0 = 1 ke setiap instance\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Menghitung theta menggunakan Normal Equation\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent adalah algoritma optimisasi generik yang bekerja dengan menyesuaikan parameter secara iteratif untuk meminimalkan fungsi biaya. Ia mengukur gradien lokal dari fungsi kesalahan terhadap parameter $\\theta$, dan bergerak ke arah gradien yang menurun. Ukuran langkahnya ditentukan oleh *learning rate*.\n",
    "\n",
    "**Varian-varian Gradient Descent:**\n",
    "* **Batch GD:** Menggunakan seluruh set pelatihan di setiap langkah. Sangat lambat pada dataset besar.\n",
    "* **Stochastic GD (SGD):** Memilih satu instance acak di setiap langkah. Jauh lebih cepat tetapi kurang teratur; solusinya bagus tetapi tidak optimal.\n",
    "* **Mini-batch GD:** Kompromi antara keduanya. Ia bekerja pada set kecil instance acak yang disebut *mini-batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Batch Gradient Descent sederhana\n",
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1)  # inisialisasi acak\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi Polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana jika data Anda lebih kompleks dari sekadar garis lurus? Kita bisa menggunakan model linear untuk data non-linear dengan cara menambahkan pangkat dari setiap fitur sebagai fitur baru, lalu melatih model linear pada set fitur yang diperluas ini. Teknik ini disebut **Regresi Polinomial**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Membuat data non-linear\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "# Menambahkan fitur polinomial\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Melatih model regresi linear pada data yang diperluas\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "print(lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Learning curves* adalah plot kinerja model pada *training set* dan *validation set* sebagai fungsi dari ukuran *training set*. Kurva ini membantu kita mendiagnosis apakah model mengalami *overfitting* atau *underfitting*.\n",
    "\n",
    "- Jika kedua kurva mencapai *plateau*, berdekatan, dan nilainya cukup tinggi, model kemungkinan besar **underfitting**.\n",
    "- Jika ada celah besar antara kurva pelatihan dan kurva validasi, model kemungkinan besar **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Linear yang Diregularisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengurangi *overfitting*, kita bisa meregularisasi model, yaitu dengan membatasi bobotnya. Tiga jenis regularisasi yang umum adalah:\n",
    "\n",
    "1.  **Ridge Regression (Regularisasi L2):** Menambahkan istilah regularisasi yang setara dengan setengah kuadrat dari norma L2 vektor bobot ke fungsi biaya. Ini mendorong bobot model sekecil mungkin.\n",
    "\n",
    "2.  **Lasso Regression (Regularisasi L1):** Menggunakan norma L1 dari vektor bobot. Lasso cenderung menghilangkan bobot dari fitur yang paling tidak penting (mengaturnya menjadi nol), sehingga secara otomatis melakukan seleksi fitur.\n",
    "\n",
    "3.  **Elastic Net:** Merupakan jalan tengah antara Ridge dan Lasso, dengan rasio campuran *r*. Ketika *r*=0, ia setara dengan Ridge, dan ketika *r*=1, ia setara dengan Lasso.\n",
    "\n",
    "**Early stopping** adalah teknik regularisasi lain di mana pelatihan dihentikan segera setelah kesalahan validasi mencapai minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi Logistik dan Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Regresi Logistik:** Digunakan untuk memperkirakan probabilitas bahwa sebuah instance termasuk dalam kelas tertentu (klasifikasi biner). Ia menghitung jumlah berbobot dari fitur masukan dan mengeluarkan *logistic* dari hasil ini.\n",
    "\n",
    "* **Regresi Softmax:** Merupakan generalisasi dari Regresi Logistik untuk mendukung banyak kelas secara langsung (klasifikasi multikelas). Ia menghitung skor untuk setiap kelas, lalu memperkirakan probabilitas setiap kelas dengan menerapkan fungsi *softmax*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
