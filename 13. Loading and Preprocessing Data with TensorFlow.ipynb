{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistem Deep Learning seringkali dilatih pada dataset yang sangat besar yang tidak muat dalam RAM. Bab ini berfokus pada cara efisien untuk memuat dan memproses data dalam jumlah besar menggunakan TensorFlow.\n",
    "\n",
    "Kita akan membahas:\n",
    "* **Data API:** Untuk membangun *pipeline* input yang sangat efisien dan dapat diskalakan.\n",
    "* **TFRecord Format:** Format biner pilihan TensorFlow untuk menyimpan dan membaca data dalam jumlah besar.\n",
    "* **Preprocessing:** Cara menangani berbagai jenis fitur (numerik, kategorikal, teks) menggunakan lapisan prapemrosesan Keras, baik saat memuat data maupun di dalam model itu sendiri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inti dari Data API adalah objek `tf.data.Dataset`, yang merepresentasikan urutan item data. TensorFlow akan menangani semua detail implementasi yang rumit seperti *multithreading*, *queuing*, *batching*, dan *prefetching*.\n",
    "\n",
    "### Chaining Transformations\n",
    "Setelah memiliki sebuah *dataset*, kita dapat menerapkan serangkaian transformasi padanya. Setiap metode transformasi mengembalikan *dataset* baru, sehingga kita bisa merantainya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Membuat dataset dari tensor di memori\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "\n",
    "# Merantai transformasi\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "\n",
    "# Iterasi melalui dataset\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and Prefetching\n",
    "* **`shuffle(buffer_size)`:** Mengacak data untuk memastikan *instance* pelatihan bersifat independen dan identik terdistribusi (IID).\n",
    "* **`map(function)`:** Menerapkan fungsi prapemrosesan pada setiap item.\n",
    "* **`prefetch(n)`:** Membuat *dataset* yang akan selalu berusaha berada `n` *batch* di depan. Ini memungkinkan CPU untuk mempersiapkan data berikutnya saat GPU sedang melatih *batch* saat ini, meningkatkan pemanfaatan perangkat keras secara dramatis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh pipeline yang lengkap\n",
    "n_steps = 10\n",
    "\n",
    "# Misalkan kita punya file CSV di path 'Chapter 13/my_data.csv'\n",
    "# Untuk tujuan konseptual, kita buat dataset dummy\n",
    "dummy_dataset = tf.data.Dataset.range(100).map(lambda x: (tf.fill([n_steps], x), x))\n",
    "\n",
    "def build_pipeline(dataset, batch_size=32):\n",
    "    dataset = dataset.shuffle(1000).repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "train_pipeline = build_pipeline(dummy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFRecord** adalah format biner sederhana dan efisien yang disarankan TensorFlow untuk menyimpan data dalam jumlah besar. Ini pada dasarnya adalah daftar catatan biner.\n",
    "\n",
    "Setiap catatan biasanya berisi *protocol buffer* yang diserialisasi. Protobuf utama yang digunakan adalah `Example`, yang merupakan struktur fleksibel untuk menyimpan fitur-fitur data. Sebuah `Example` berisi pemetaan dari nama fitur ke nilainya (`BytesList`, `FloatList`, atau `Int64List`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menulis ke file TFRecord\n",
    "with tf.io.TFRecordWriter(\"Chapter 13/my_data.tfrecord\") as f:\n",
    "    f.write(b\"Ini adalah catatan pertama\")\n",
    "    f.write(b\"Dan ini adalah catatan kedua\")\n",
    "\n",
    "# Membaca dari file TFRecord\n",
    "filepaths = [\"Chapter 13/my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelum data dimasukkan ke jaringan saraf, semua fitur harus diubah menjadi numerik dan seringkali perlu dinormalisasi. Ini bisa dilakukan di awal saat membuat file data, saat memuat dengan Data API, atau sebagai lapisan prapemrosesan langsung di dalam model.\n",
    "\n",
    "### Encoding Categorical Features\n",
    "* **One-Hot Encoding:** Cocok untuk fitur dengan sedikit kategori.\n",
    "* **Embeddings:** Cocok untuk fitur dengan banyak kategori (misalnya, ribuan kata dalam kamus). Embedding adalah vektor padat yang dapat dilatih yang merepresentasikan sebuah kategori. Selama pelatihan, model akan belajar representasi yang berguna untuk setiap kategori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Konseptual: Menggunakan Embedding Layer\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # Misalkan input adalah ID kata (integer)\n",
    "    keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk dataset yang umum digunakan, proyek **TFDS** membuatnya sangat mudah untuk diunduh dan digunakan. Pustaka ini menangani pengunduhan, pemisahan, dan penyajian data sebagai objek `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Memuat dataset MNIST, sudah siap pakai\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "mnist_train = dataset[\"train\"].prefetch(1)\n",
    "\n",
    "print(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
