{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bab ini melanjutkan eksplorasi RNN, kali ini dengan fokus pada **Natural Language Processing (NLP)**, sebuah bidang yang bertujuan untuk memungkinkan komputer memahami dan menghasilkan bahasa manusia.\n",
    "\n",
    "Kita akan membahas beberapa aplikasi dan arsitektur canggih:\n",
    "* **Text Generation:** Menggunakan Character-RNN untuk menghasilkan teks yang mirip dengan karya Shakespeare.\n",
    "* **Sentiment Analysis:** Mengklasifikasikan teks (misalnya, ulasan film) sebagai positif atau negatif.\n",
    "* **Neural Machine Translation (NMT):** Membangun model **Encoder-Decoder** untuk menerjemahkan kalimat dari satu bahasa ke bahasa lain.\n",
    "* **Attention Mechanisms:** Teknik revolusioner yang memungkinkan model untuk fokus pada bagian-bagian relevan dari input, mengatasi keterbatasan memori RNN.\n",
    "* **The Transformer:** Arsitektur yang sangat sukses yang sepenuhnya bergantung pada *attention*, tanpa menggunakan lapisan rekuren atau konvolusional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salah satu aplikasi menarik dari RNN adalah menghasilkan teks. Dengan melatih sebuah model untuk memprediksi karakter berikutnya dalam sebuah kalimat, kita dapat menggunakannya untuk menghasilkan teks baru, karakter demi karakter.\n",
    "\n",
    "Prosesnya melibatkan:\n",
    "1. **Membuat Dataset:** Mengambil teks panjang (misalnya, semua karya Shakespeare) dan mengubahnya menjadi urutan ID karakter.\n",
    "2. **Memotong Data:** Membagi urutan panjang menjadi banyak jendela teks yang lebih kecil (*windows*).\n",
    "3. **Membangun Model:** Menggunakan lapisan RNN seperti GRU atau LSTM untuk memprediksi karakter berikutnya.\n",
    "4. **Menghasilkan Teks:** Memberi model sebuah teks awal, lalu berulang kali memprediksi karakter berikutnya dan menambahkannya ke teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Asumsikan max_id adalah jumlah karakter unik\n",
    "max_id = 39 \n",
    "\n",
    "# Membangun model Char-RNN\n",
    "model_char_rnn = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                   dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                   dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model_char_rnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model_char_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis sentimen adalah tugas klasifikasi teks, misalnya menentukan apakah sebuah ulasan film positif atau negatif. Alih-alih memproses karakter per karakter, kita biasanya memproses kata per kata.\n",
    "\n",
    "Ini memerlukan dua langkah penting:\n",
    "1. **Tokenization:** Memecah teks menjadi kata-kata atau sub-kata (*tokens*).\n",
    "2. **Embedding:** Mengubah setiap token menjadi representasi vektor numerik yang padat. Lapisan `Embedding` di Keras sangat cocok untuk ini. Ia belajar representasi yang berguna untuk setiap kata selama pelatihan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Memuat dataset IMDb reviews\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "train_set = datasets[\"train\"].batch(32).prefetch(1)\n",
    "\n",
    "# Konseptual: Model Analisis Sentimen\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "\n",
    "model_sentiment = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# history = model_sentiment.fit(train_set, epochs=5) # Pelatihan akan membutuhkan prapemrosesan teks yang sesuai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Encoder–Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk tugas seperti terjemahan mesin, di mana panjang output bisa berbeda dari panjang input, arsitektur **Encoder–Decoder** sangat efektif.\n",
    "\n",
    "* **Encoder:** Sebuah RNN yang membaca kalimat sumber dan mengubahnya menjadi satu representasi vektor (disebut *context vector* atau *thought vector*).\n",
    "* **Decoder:** Sebuah RNN yang mengambil *context vector* dan menghasilkan terjemahan kata demi kata.\n",
    "\n",
    "Masalah dengan arsitektur ini adalah *context vector* menjadi hambatan (*bottleneck*) informasi, terutama untuk kalimat yang panjang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Mechanism** adalah terobosan yang mengatasi masalah *bottleneck* pada model Encoder-Decoder. Ide utamanya adalah memungkinkan *decoder* untuk \"memperhatikan\" bagian-bagian yang berbeda dari kalimat sumber pada setiap langkah waktu saat menghasilkan output.\n",
    "\n",
    "Ini tidak hanya meningkatkan kinerja secara dramatis pada kalimat panjang tetapi juga memberikan *interpretability*: kita bisa memvisualisasikan bagian mana dari input yang menjadi fokus model saat menghasilkan output tertentu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahun 2017, sebuah makalah berjudul \"Attention Is All You Need\" memperkenalkan arsitektur **Transformer**, yang sepenuhnya meninggalkan lapisan rekuren dan hanya mengandalkan *attention mechanism* (khususnya **self-attention**).\n",
    "\n",
    "Komponen utamanya adalah **Multi-Head Attention**, yang memungkinkan model untuk secara bersamaan memperhatikan informasi dari *subspace* representasi yang berbeda. Karena tidak ada rekurensi, Transformer menambahkan **Positional Embeddings** ke input untuk memberikan informasi tentang urutan kata.\n",
    "\n",
    "Arsitektur ini lebih mudah diparalelkan daripada RNN dan telah menjadi dasar bagi model bahasa canggih seperti BERT dan GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisasi Pelatihan (Konseptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data dummy untuk history pelatihan\n",
    "dummy_history = {\n",
    "    'loss': [1.5, 0.8, 0.5, 0.3, 0.2],\n",
    "    'accuracy': [0.5, 0.7, 0.8, 0.9, 0.95],\n",
    "    'val_loss': [1.6, 0.9, 0.6, 0.4, 0.3],\n",
    "    'val_accuracy': [0.48, 0.68, 0.78, 0.88, 0.92]\n",
    "}\n",
    "\n",
    "pd.DataFrame(dummy_history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 2)\n",
    "plt.title(\"Contoh Kurva Pembelajaran untuk Analisis Sentimen\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
